{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import io\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import bson\n",
    "from skimage.data import imread\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import cv2\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Conv2D, Dense, Dropout, Flatten, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Decode bson data from a file to multiple documents as a generator. \n",
    "Reads from the file object in chunks and parses bson in chunks, yielding one document at a time.\n",
    "'''\n",
    "data = bson.decode_file_iter(open('input/train_example.bson', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def data_generator(batch_size=4):\n",
    "    print('in data_generator')\n",
    "    count_product = 0\n",
    "    images = []\n",
    "    y_label = []\n",
    "    \n",
    "    while True:\n",
    "        count = 0\n",
    "        print('count: ' + str(count))\n",
    "        for c, d in enumerate(data):\n",
    "            category_id = d['category_id']\n",
    "            for e, pic in enumerate(d['imgs']):\n",
    "                picture = imread(io.BytesIO(pic['picture']))\n",
    "                images.append(picture)\n",
    "                y_label.append(category_id)\n",
    "                count = count + 1\n",
    "            if count >= batch_size:\n",
    "                break;\n",
    "\n",
    "        images = np.asarray(images)                \n",
    "        print('images.shape: ' + print(images.shape))\n",
    "        print(y_label)\n",
    "        \n",
    "        y_label = to_categorical(y_label, 36)\n",
    "\n",
    "        yield (images, y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def data_gen(generator=data):\n",
    "    while True:\n",
    "        images=[]\n",
    "        category=[]\n",
    "        prod_to_category = dict()\n",
    "        images_per_category=[]\n",
    "        flag=0\n",
    "\n",
    "        for c, d in enumerate(data):\n",
    "            product_id = d['_id']\n",
    "            category_id = d['category_id'] # This won't be in Test data\n",
    "            #prod_to_category[product_id] = category_id\n",
    "            for e, pic in enumerate(d['imgs']):\n",
    "                category.append(category_id)\n",
    "                picture = imread(io.BytesIO(pic['picture']))\n",
    "                #picture=pic['picture']\n",
    "                images.append(picture)\n",
    "        yield np.array(images), encode_labels(np.asarray(category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = bson.decode_file_iter(open('input/train_example.bson', 'rb'))\n",
    "i, c = data_gen(generator=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_labels(labels, num_classes=523):\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(labels)\n",
    "\n",
    "    encoded_Y = encoder.transform(labels)\n",
    "    return to_categorical(encoded_Y, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imread(buf):\n",
    "    return cv2.imdecode(np.frombuffer(buf, np.uint8), cv2.IMREAD_ANYCOLOR)\n",
    "\n",
    "def img2feat(im):\n",
    "    x = cv2.resize(im, (32, 32), interpolation=cv2.INTER_AREA)\n",
    "    x = np.float32(x) / 255\n",
    "    return x\n",
    "\n",
    "def datagen(batch_size):\n",
    "    data = bson.decode_file_iter(open('input/train.bson', 'rb'))\n",
    "    \n",
    "    while True:\n",
    "        count = 0\n",
    "        X = np.zeros((batch_size, 32, 32, 3), dtype=np.float32)\n",
    "        images=[]\n",
    "        category=[]\n",
    "        \n",
    "#        prod_to_category = dict()\n",
    "#        images_per_category=[]\n",
    "\n",
    "        for c, d in enumerate(data):\n",
    "            if count >= batch_size:\n",
    "                X = np.asarray(images)\n",
    "                X = X.reshape(X.shape[0], -1)\n",
    "#                print('X.shape: ' + str(X.shape))\n",
    "                yield X, encode_labels(np.asarray(category))\n",
    "                \n",
    "                count = 0\n",
    "                X = np.zeros((batch_size, 32, 32, 3), dtype=np.float32)\n",
    "                images=[]\n",
    "                category=[]\n",
    "            \n",
    "            else:    \n",
    "                product_id = d['_id']\n",
    "                category_id = d['category_id'] # This won't be in Test data\n",
    "\n",
    "    #            print(\"category_id: \" + str(category_id))\n",
    "\n",
    "                #prod_to_category[product_id] = category_id\n",
    "                for e, pic in enumerate(d['imgs']):\n",
    "                    category.append(category_id)\n",
    "                    picture = imread(io.BytesIO(pic['picture']).getbuffer())\n",
    "                    images.append(img2feat(picture))\n",
    "                    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "fc1 (Dense)                  (None, 1024)              3146752   \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           (None, 523)               536075    \n",
      "=================================================================\n",
      "Total params: 3,682,827\n",
      "Trainable params: 3,682,827\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1024, activation='relu', input_dim=3072, name='fc1'))\n",
    "model.add(Dense(523, activation='softmax', name='classifier'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Epoch 00000: val_loss improved from inf to 15.32017, saving model to cdiscount\n",
      "1s - loss: 14.7408 - acc: 0.0855 - val_loss: 15.3202 - val_acc: 0.0495\n",
      "Epoch 2/10\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 14.9925 - acc: 0.0698 - val_loss: 15.7989 - val_acc: 0.0198\n",
      "Epoch 3/10\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 14.8172 - acc: 0.0807 - val_loss: 15.6346 - val_acc: 0.0300\n",
      "Epoch 4/10\n",
      "Epoch 00003: val_loss improved from 15.32017 to 12.79967, saving model to cdiscount\n",
      "1s - loss: 14.6895 - acc: 0.0886 - val_loss: 12.7997 - val_acc: 0.2059\n",
      "Epoch 5/10\n",
      "Epoch 00004: val_loss improved from 12.79967 to 12.44764, saving model to cdiscount\n",
      "0s - loss: 14.3049 - acc: 0.1125 - val_loss: 12.4476 - val_acc: 0.2277\n",
      "Epoch 6/10\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 14.6863 - acc: 0.0888 - val_loss: 15.7957 - val_acc: 0.0200\n",
      "Epoch 7/10\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 14.1557 - acc: 0.1217 - val_loss: 15.7957 - val_acc: 0.0200\n",
      "Epoch 8/10\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 14.2379 - acc: 0.1167 - val_loss: 15.6393 - val_acc: 0.0297\n",
      "Model took 7.49 seconds to train\n"
     ]
    }
   ],
   "source": [
    "#data = bson.decode_file_iter(open('input/train_example.bson', 'rb'))\n",
    "\n",
    "#model.fit_generator(generator=datagen(batch_size=64), steps_per_epoch=2000, epochs=1, verbose=2)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "model_checkpoint = ModelCheckpoint(filepath='cdiscount', verbose=1, save_best_only=True)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "model.fit_generator(generator=datagen(batch_size=50), steps_per_epoch=20, validation_data=datagen(batch_size=50), \n",
    "                    validation_steps=2, callbacks=[early_stopping, model_checkpoint], epochs=10, verbose=2)\n",
    "end = time.time()\n",
    "print(\"Model took %0.2f seconds to train\"%(end - start))\n",
    "\n",
    "# Re-instantiate model to the best model saved\n",
    "#model = load_model('cdiscount_vgg16')\n",
    "\n",
    "#y_pred = model.predict(X_test, batch_size=64)\n",
    "#score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "\n",
    "#print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

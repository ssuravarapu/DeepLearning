{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Flatten, Embedding, Dropout, BatchNormalization, Conv1D, MaxPool1D, SpatialDropout1D, LSTM\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.preprocessing import sequence\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>directed_at</th>\n",
       "      <th>tweet_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>@teachntech00 New iPad Apps For #SpeechTherapy...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   X                                         tweet_text directed_at  \\\n",
       "0  1  .@wesley83 I have a 3G iPhone. After 3 hrs twe...      iPhone   \n",
       "1  3  @swonderlin Can not wait for #iPad 2 also. The...        iPad   \n",
       "2  5  @sxtxstate great stuff on Fri #SXSW: Marissa M...      Google   \n",
       "3  6  @teachntech00 New iPad Apps For #SpeechTherapy...         NaN   \n",
       "4  7                                                NaN         NaN   \n",
       "\n",
       "  tweet_sentiment  \n",
       "0        negative  \n",
       "1        positive  \n",
       "2        positive  \n",
       "3         neutral  \n",
       "4             NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8936, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_tweets = pd.read_csv('negative_tweet_supplement.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tweet_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>WTF MY BATTERY WAS 31% ONE SECOND AGO AND NOW ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>@apple Contact sync between Yosemite and iOS8 ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>WARNING IF YOU BUY AN IPHONE 5S UNLOCKED FROM ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>@Apple, For the love of GAWD, CENTER the '1'on...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>i get the storage almost full notification lit...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    X                                         tweet_text tweet_sentiment\n",
       "0  11  WTF MY BATTERY WAS 31% ONE SECOND AGO AND NOW ...        negative\n",
       "1  15  @apple Contact sync between Yosemite and iOS8 ...        negative\n",
       "2  17  WARNING IF YOU BUY AN IPHONE 5S UNLOCKED FROM ...        negative\n",
       "3  24  @Apple, For the love of GAWD, CENTER the '1'on...        negative\n",
       "4  25  i get the storage almost full notification lit...        negative"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1219, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11567, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airline_tweets = pd.read_csv('airline_tweets.csv')\n",
    "airline_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tweet_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text tweet_sentiment\n",
       "0  @VirginAmerica plus you've added commercials t...        positive\n",
       "1  @VirginAmerica it's really aggressive to blast...        negative\n",
       "2  @VirginAmerica and it's a really big bad thing...        negative\n",
       "3  @VirginAmerica seriously would pay $30 a fligh...        negative\n",
       "4  @VirginAmerica yes, nearly every time I fly VX...        positive"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airline_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10262, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debate_tweets = pd.read_csv('gop_debate_tweets.csv')\n",
    "debate_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tweet_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @NancyLeeGrahn: How did everyone feel about...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @ScottWalker: Didn't catch the full #GOPdeb...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @TJMShow: No mention of Tamir Rice and the ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @RobGeorge: That Carly Fiorina is trending ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @DanScavino: #GOPDebate w/ @realDonaldTrump...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text tweet_sentiment\n",
       "0  RT @NancyLeeGrahn: How did everyone feel about...         neutral\n",
       "1  RT @ScottWalker: Didn't catch the full #GOPdeb...        positive\n",
       "2  RT @TJMShow: No mention of Tamir Rice and the ...         neutral\n",
       "3  RT @RobGeorge: That Carly Fiorina is trending ...        positive\n",
       "4  RT @DanScavino: #GOPDebate w/ @realDonaldTrump...        positive"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debate_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = tweets.drop('directed_at', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_tweets = tweets.append(negative_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10155, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_tweets = all_tweets.drop('X', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10155, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_tweets = all_tweets.append(airline_tweets)\n",
    "all_tweets = all_tweets.append(debate_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31984, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ingest(all_tweets):\n",
    "    all_tweets = all_tweets[all_tweets['tweet_text'].isnull() == False]\n",
    "    all_tweets = all_tweets[all_tweets['tweet_sentiment'].isnull() == False]\n",
    "    return all_tweets\n",
    "\n",
    "all_tweets = ingest(all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31983, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15138, 2), (10397, 2), (6448, 2))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets[all_tweets['tweet_sentiment'] == 'negative'].shape, all_tweets[all_tweets['tweet_sentiment'] == 'neutral'].shape, all_tweets[all_tweets['tweet_sentiment'] == 'positive'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tknzr):\n",
    "    sw = stopwords.words('english')\n",
    "    \n",
    "    words_to_remove = []\n",
    "    for word, idx in tknzr.word_index.items():\n",
    "        if word in sw:\n",
    "            words_to_remove.append(word)\n",
    "\n",
    "    print(len(tknzr.word_index))        \n",
    "    for w in words_to_remove:\n",
    "        tknzr.word_index.pop(w)\n",
    "\n",
    "    print(len(tknzr.word_index))\n",
    "    \n",
    "    return tknzr\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34043\n",
      "33908\n"
     ]
    }
   ],
   "source": [
    "#all_tweets['tweet_text'] = all_tweets['tweet_text'].apply(lambda x: x.lower())\n",
    "#all_tweets['tweet_text'] = all_tweets['tweet_text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','', x)))\n",
    "\n",
    "max_features = 5000\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
    "tokenizer.fit_on_texts(all_tweets['tweet_text'].values)\n",
    "tokenizer = remove_stopwords(tokenizer)\n",
    "\n",
    "X = tokenizer.texts_to_sequences(all_tweets['tweet_text'].values)\n",
    "X = pad_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31983, 46)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0, 1721,   53,  134,  382,  963, 2639,   84, 1290,   88,\n",
       "        666,    3], dtype=int32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = pd.get_dummies(all_tweets['tweet_sentiment']).values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28784, 46), (28784, 3), (3199, 46), (3199, 3))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "       1542, 2306, 3002, 1570,   31, 3002,   20,  109,    8,  808,  178,\n",
       "          3,   22], dtype=int32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1], dtype=uint8)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three models: Simple Neural Network, CNN, and LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single hidden layer NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_dim = 32\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_dim, input_length=X_train.shape[1]))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 46, 32)            160000    \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 46, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1472)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 100)               147300    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 308,003\n",
      "Trainable params: 307,803\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25905 samples, validate on 2879 samples\n",
      "Epoch 1/30\n",
      "Epoch 00000: val_loss improved from inf to 0.88087, saving model to tweet_sentiment_simple\n",
      "3s - loss: 1.0201 - acc: 0.5316 - val_loss: 0.8809 - val_acc: 0.6186\n",
      "Epoch 2/30\n",
      "Epoch 00001: val_loss improved from 0.88087 to 0.75575, saving model to tweet_sentiment_simple\n",
      "1s - loss: 0.8158 - acc: 0.6340 - val_loss: 0.7558 - val_acc: 0.6384\n",
      "Epoch 3/30\n",
      "Epoch 00002: val_loss improved from 0.75575 to 0.74093, saving model to tweet_sentiment_simple\n",
      "1s - loss: 0.7651 - acc: 0.6473 - val_loss: 0.7409 - val_acc: 0.6481\n",
      "Epoch 4/30\n",
      "Epoch 00003: val_loss did not improve\n",
      "1s - loss: 0.7347 - acc: 0.6566 - val_loss: 0.7473 - val_acc: 0.6422\n",
      "Epoch 5/30\n",
      "Epoch 00004: val_loss did not improve\n",
      "1s - loss: 0.7142 - acc: 0.6683 - val_loss: 0.7417 - val_acc: 0.6523\n",
      "Epoch 6/30\n",
      "Epoch 00005: val_loss improved from 0.74093 to 0.73834, saving model to tweet_sentiment_simple\n",
      "1s - loss: 0.6910 - acc: 0.6833 - val_loss: 0.7383 - val_acc: 0.6586\n",
      "Epoch 7/30\n",
      "Epoch 00006: val_loss improved from 0.73834 to 0.72339, saving model to tweet_sentiment_simple\n",
      "1s - loss: 0.6625 - acc: 0.7048 - val_loss: 0.7234 - val_acc: 0.6804\n",
      "Epoch 8/30\n",
      "Epoch 00007: val_loss improved from 0.72339 to 0.70300, saving model to tweet_sentiment_simple\n",
      "1s - loss: 0.6299 - acc: 0.7268 - val_loss: 0.7030 - val_acc: 0.7016\n",
      "Epoch 9/30\n",
      "Epoch 00008: val_loss improved from 0.70300 to 0.69354, saving model to tweet_sentiment_simple\n",
      "1s - loss: 0.6181 - acc: 0.7387 - val_loss: 0.6935 - val_acc: 0.7121\n",
      "Epoch 10/30\n",
      "Epoch 00009: val_loss did not improve\n",
      "1s - loss: 0.5896 - acc: 0.7536 - val_loss: 0.7241 - val_acc: 0.7034\n",
      "Epoch 11/30\n",
      "Epoch 00010: val_loss did not improve\n",
      "1s - loss: 0.5690 - acc: 0.7648 - val_loss: 0.7107 - val_acc: 0.7051\n",
      "Epoch 12/30\n",
      "Epoch 00011: val_loss did not improve\n",
      "1s - loss: 0.5520 - acc: 0.7717 - val_loss: 0.7283 - val_acc: 0.7020\n",
      "Model took 25.46 seconds to train\n",
      "2496/3199 [======================>.......] - ETA: 0s[0.68505909918769892, 0.7027195999867546]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "model_checkpoint = ModelCheckpoint(filepath='tweet_sentiment_simple', verbose=1, save_best_only=True)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, validation_split=0.1, epochs=30, verbose=2, \n",
    "          callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "end = time.time()\n",
    "print(\"Model took %0.2f seconds to train\"%(end - start))\n",
    "\n",
    "# Re-instantiate model to the best model saved\n",
    "model = load_model('tweet_sentiment_simple')\n",
    "\n",
    "y_pred = model.predict(X_test, batch_size=64)\n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single conv layer with max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(max_features, embed_dim, input_length=X_train.shape[1]))\n",
    "model2.add(SpatialDropout1D(0.3))\n",
    "model2.add(Dropout(0.6))\n",
    "model2.add(Conv1D(32, 5, padding='same', activation='relu'))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(MaxPool1D(5))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(100, activation='relu'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Dropout(0.7))\n",
    "model2.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 46, 128)           640000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_3 (Spatial (None, 46, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 46, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 46, 32)            20512     \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 46, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 9, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 288)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 100)               28900     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 690,115\n",
      "Trainable params: 689,915\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25905 samples, validate on 2879 samples\n",
      "Epoch 1/30\n",
      "Epoch 00000: val_loss improved from inf to 0.89667, saving model to tweet_sentiment_cnn\n",
      "10s - loss: 1.0730 - acc: 0.5185 - val_loss: 0.8967 - val_acc: 0.5992\n",
      "Epoch 2/30\n",
      "Epoch 00001: val_loss improved from 0.89667 to 0.87036, saving model to tweet_sentiment_cnn\n",
      "10s - loss: 0.8124 - acc: 0.6381 - val_loss: 0.8704 - val_acc: 0.5929\n",
      "Epoch 3/30\n",
      "Epoch 00002: val_loss improved from 0.87036 to 0.82251, saving model to tweet_sentiment_cnn\n",
      "10s - loss: 0.7586 - acc: 0.6521 - val_loss: 0.8225 - val_acc: 0.6144\n",
      "Epoch 4/30\n",
      "Epoch 00003: val_loss improved from 0.82251 to 0.81529, saving model to tweet_sentiment_cnn\n",
      "9s - loss: 0.7294 - acc: 0.6590 - val_loss: 0.8153 - val_acc: 0.6259\n",
      "Epoch 5/30\n",
      "Epoch 00004: val_loss improved from 0.81529 to 0.78933, saving model to tweet_sentiment_cnn\n",
      "9s - loss: 0.7093 - acc: 0.6703 - val_loss: 0.7893 - val_acc: 0.6322\n",
      "Epoch 6/30\n",
      "Epoch 00005: val_loss improved from 0.78933 to 0.76904, saving model to tweet_sentiment_cnn\n",
      "9s - loss: 0.6848 - acc: 0.6886 - val_loss: 0.7690 - val_acc: 0.6464\n",
      "Epoch 7/30\n",
      "Epoch 00006: val_loss did not improve\n",
      "10s - loss: 0.6515 - acc: 0.7150 - val_loss: 0.7887 - val_acc: 0.6485\n",
      "Epoch 8/30\n",
      "Epoch 00007: val_loss improved from 0.76904 to 0.74413, saving model to tweet_sentiment_cnn\n",
      "10s - loss: 0.6221 - acc: 0.7382 - val_loss: 0.7441 - val_acc: 0.6763\n",
      "Epoch 9/30\n",
      "Epoch 00008: val_loss improved from 0.74413 to 0.72325, saving model to tweet_sentiment_cnn\n",
      "9s - loss: 0.5998 - acc: 0.7529 - val_loss: 0.7233 - val_acc: 0.6898\n",
      "Epoch 10/30\n",
      "Epoch 00009: val_loss improved from 0.72325 to 0.71738, saving model to tweet_sentiment_cnn\n",
      "9s - loss: 0.5801 - acc: 0.7642 - val_loss: 0.7174 - val_acc: 0.6863\n",
      "Epoch 11/30\n",
      "Epoch 00010: val_loss improved from 0.71738 to 0.71532, saving model to tweet_sentiment_cnn\n",
      "11s - loss: 0.5641 - acc: 0.7680 - val_loss: 0.7153 - val_acc: 0.6895\n",
      "Epoch 12/30\n",
      "Epoch 00011: val_loss did not improve\n",
      "9s - loss: 0.5502 - acc: 0.7757 - val_loss: 0.7203 - val_acc: 0.6881\n",
      "Epoch 13/30\n",
      "Epoch 00012: val_loss did not improve\n",
      "9s - loss: 0.5384 - acc: 0.7815 - val_loss: 0.7268 - val_acc: 0.6839\n",
      "Epoch 14/30\n",
      "Epoch 00013: val_loss did not improve\n",
      "10s - loss: 0.5234 - acc: 0.7876 - val_loss: 0.7223 - val_acc: 0.6853\n",
      "Model took 142.03 seconds to train\n",
      "2688/3199 [========================>.....] - ETA: 0s[0.69198439817348101, 0.70396999073386302]\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "model_checkpoint = ModelCheckpoint(filepath='tweet_sentiment_cnn', verbose=1, save_best_only=True)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "model2.fit(X_train, y_train, batch_size=64, validation_split=0.1, epochs=30, verbose=2, \n",
    "          callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "end = time.time()\n",
    "print(\"Model took %0.2f seconds to train\"%(end - start))\n",
    "\n",
    "# Re-instantiate model to the best model saved\n",
    "model2 = load_model('tweet_sentiment_cnn')\n",
    "\n",
    "y_pred = model2.predict(X_test, batch_size=64)\n",
    "score = model2.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_dim = 32\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Embedding(max_features, embed_dim, input_length=X_train.shape[1], embeddings_regularizer=l2(0.0001)))\n",
    "model3.add(SpatialDropout1D(0.2))\n",
    "model3.add(Dropout(0.6))\n",
    "\n",
    "model3.add(Conv1D(128, 5, activation='relu'))\n",
    "model3.add(Dropout(0.6))\n",
    "model3.add(MaxPool1D())\n",
    "\n",
    "model3.add(Flatten())\n",
    "\n",
    "model3.add(Dense(256, activation='relu', kernel_regularizer=l2(0.0001)))\n",
    "model3.add(Dropout(0.7))\n",
    "model3.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 46, 32)            160000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_4 (Spatial (None, 46, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 46, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 42, 128)           20608     \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 42, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 256)               688384    \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 869,763\n",
      "Trainable params: 869,763\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model3.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001, decay=1e-7), metrics=['accuracy'])\n",
    "model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25905 samples, validate on 2879 samples\n",
      "Epoch 1/30\n",
      "Epoch 00000: val_loss improved from inf to 0.82521, saving model to tweet_sentiment_multicnn\n",
      "11s - loss: 0.9132 - acc: 0.5883 - val_loss: 0.8252 - val_acc: 0.6464\n",
      "Epoch 2/30\n",
      "Epoch 00001: val_loss improved from 0.82521 to 0.78536, saving model to tweet_sentiment_multicnn\n",
      "10s - loss: 0.7755 - acc: 0.6527 - val_loss: 0.7854 - val_acc: 0.6520\n",
      "Epoch 3/30\n",
      "Epoch 00002: val_loss improved from 0.78536 to 0.76476, saving model to tweet_sentiment_multicnn\n",
      "11s - loss: 0.7437 - acc: 0.6680 - val_loss: 0.7648 - val_acc: 0.6516\n",
      "Epoch 4/30\n",
      "Epoch 00003: val_loss improved from 0.76476 to 0.73620, saving model to tweet_sentiment_multicnn\n",
      "11s - loss: 0.7107 - acc: 0.7047 - val_loss: 0.7362 - val_acc: 0.7051\n",
      "Epoch 5/30\n",
      "Epoch 00004: val_loss improved from 0.73620 to 0.73070, saving model to tweet_sentiment_multicnn\n",
      "10s - loss: 0.6819 - acc: 0.7295 - val_loss: 0.7307 - val_acc: 0.7002\n",
      "Epoch 6/30\n",
      "Epoch 00005: val_loss did not improve\n",
      "10s - loss: 0.6669 - acc: 0.7417 - val_loss: 0.7340 - val_acc: 0.7155\n",
      "Epoch 7/30\n",
      "Epoch 00006: val_loss did not improve\n",
      "10s - loss: 0.6586 - acc: 0.7472 - val_loss: 0.7372 - val_acc: 0.7107\n",
      "Epoch 8/30\n",
      "Epoch 00007: val_loss did not improve\n",
      "10s - loss: 0.6510 - acc: 0.7534 - val_loss: 0.7344 - val_acc: 0.7100\n",
      "Model took 86.82 seconds to train\n",
      "3072/3199 [===========================>..] - ETA: 0s[0.71785494758770518, 0.70865895575603566]\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "model_checkpoint = ModelCheckpoint(filepath='tweet_sentiment_multicnn', verbose=1, save_best_only=True)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "model3.fit(X_train, y_train, batch_size=64, validation_split=0.1, epochs=30, verbose=2, \n",
    "          callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "end = time.time()\n",
    "print(\"Model took %0.2f seconds to train\"%(end - start))\n",
    "\n",
    "# Re-instantiate model to the best model saved\n",
    "model3 = load_model('tweet_sentiment_multicnn')\n",
    "\n",
    "y_pred = model3.predict(X_test, batch_size=64)\n",
    "score = model3.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 46, 96)            480000    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 46, 96)            384       \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 46, 96)            0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 96)                74112     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 125)               12125     \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 125)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 3)                 378       \n",
      "=================================================================\n",
      "Total params: 566,999\n",
      "Trainable params: 566,807\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 96\n",
    "lstm_out = 96\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Embedding(max_features, embed_dim, input_length = X.shape[1]))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(LSTM(lstm_out, dropout=0.4, recurrent_dropout=0.3, activation='relu'))\n",
    "model3.add(Dense(125, activation='relu'))\n",
    "model3.add(Dropout(0.7))\n",
    "model3.add(Dense(3, activation='softmax'))\n",
    "model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25905 samples, validate on 2879 samples\n",
      "Epoch 1/30\n",
      "Epoch 00000: val_loss improved from inf to 1.01317, saving model to tweet_sentiment_lstm\n",
      "62s - loss: 0.8941 - acc: 0.5929 - val_loss: 1.0132 - val_acc: 0.6162\n",
      "Epoch 2/30\n",
      "Epoch 00001: val_loss improved from 1.01317 to 0.73060, saving model to tweet_sentiment_lstm\n",
      "58s - loss: 0.7513 - acc: 0.6593 - val_loss: 0.7306 - val_acc: 0.6822\n",
      "Epoch 3/30\n",
      "Epoch 00002: val_loss improved from 0.73060 to 0.69222, saving model to tweet_sentiment_lstm\n",
      "64s - loss: 0.6769 - acc: 0.7086 - val_loss: 0.6922 - val_acc: 0.7016\n",
      "Epoch 4/30\n",
      "Epoch 00003: val_loss did not improve\n",
      "54s - loss: 0.6285 - acc: 0.7360 - val_loss: 0.7070 - val_acc: 0.7023\n",
      "Epoch 5/30\n",
      "Epoch 00004: val_loss improved from 0.69222 to 0.68446, saving model to tweet_sentiment_lstm\n",
      "54s - loss: 0.5932 - acc: 0.7538 - val_loss: 0.6845 - val_acc: 0.7051\n",
      "Epoch 6/30\n",
      "Epoch 00005: val_loss did not improve\n",
      "54s - loss: 0.5701 - acc: 0.7644 - val_loss: 0.7052 - val_acc: 0.7068\n",
      "Epoch 7/30\n",
      "Epoch 00006: val_loss did not improve\n",
      "54s - loss: 0.5522 - acc: 0.7692 - val_loss: 0.7346 - val_acc: 0.7100\n",
      "Epoch 8/30\n",
      "Epoch 00007: val_loss did not improve\n",
      "55s - loss: 0.5366 - acc: 0.7773 - val_loss: 0.7392 - val_acc: 0.7009\n",
      "Model took 460.76 seconds to train\n",
      "3199/3199 [==============================] - 3s     \n",
      "[0.66375873770032612, 0.71428571409939157]\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "model_checkpoint = ModelCheckpoint(filepath='tweet_sentiment_lstm', verbose=1, save_best_only=True)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "model3.fit(X_train, y_train, batch_size=64, validation_split=0.1, epochs=30, verbose=2, \n",
    "          callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "end = time.time()\n",
    "print(\"Model took %0.2f seconds to train\"%(end - start))\n",
    "\n",
    "# Re-instantiate model to the best model saved\n",
    "model3 = load_model('tweet_sentiment_lstm')\n",
    "\n",
    "y_pred = model3.predict(X_test, batch_size=64)\n",
    "score = model3.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Pre-trained Model (GloVe-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join('glove.twitter.27B', 'glove.twitter.27B.25d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34043 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_dim = 32\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embed_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

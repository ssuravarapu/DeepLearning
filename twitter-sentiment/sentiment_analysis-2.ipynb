{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Flatten, Embedding, Dropout, BatchNormalization, Conv1D, MaxPool1D, SpatialDropout1D, LSTM\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.preprocessing import sequence\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>directed_at</th>\n",
       "      <th>tweet_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>@teachntech00 New iPad Apps For #SpeechTherapy...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   X                                         tweet_text directed_at  \\\n",
       "0  1  .@wesley83 I have a 3G iPhone. After 3 hrs twe...      iPhone   \n",
       "1  3  @swonderlin Can not wait for #iPad 2 also. The...        iPad   \n",
       "2  5  @sxtxstate great stuff on Fri #SXSW: Marissa M...      Google   \n",
       "3  6  @teachntech00 New iPad Apps For #SpeechTherapy...         NaN   \n",
       "4  7                                                NaN         NaN   \n",
       "\n",
       "  tweet_sentiment  \n",
       "0        negative  \n",
       "1        positive  \n",
       "2        positive  \n",
       "3         neutral  \n",
       "4             NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8936, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_tweets = pd.read_csv('negative_tweet_supplement.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tweet_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>WTF MY BATTERY WAS 31% ONE SECOND AGO AND NOW ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>@apple Contact sync between Yosemite and iOS8 ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>WARNING IF YOU BUY AN IPHONE 5S UNLOCKED FROM ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>@Apple, For the love of GAWD, CENTER the '1'on...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>i get the storage almost full notification lit...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    X                                         tweet_text tweet_sentiment\n",
       "0  11  WTF MY BATTERY WAS 31% ONE SECOND AGO AND NOW ...        negative\n",
       "1  15  @apple Contact sync between Yosemite and iOS8 ...        negative\n",
       "2  17  WARNING IF YOU BUY AN IPHONE 5S UNLOCKED FROM ...        negative\n",
       "3  24  @Apple, For the love of GAWD, CENTER the '1'on...        negative\n",
       "4  25  i get the storage almost full notification lit...        negative"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1219, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11567, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airline_tweets = pd.read_csv('airline_tweets.csv')\n",
    "airline_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tweet_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text tweet_sentiment\n",
       "0  @VirginAmerica plus you've added commercials t...        positive\n",
       "1  @VirginAmerica it's really aggressive to blast...        negative\n",
       "2  @VirginAmerica and it's a really big bad thing...        negative\n",
       "3  @VirginAmerica seriously would pay $30 a fligh...        negative\n",
       "4  @VirginAmerica yes, nearly every time I fly VX...        positive"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airline_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10262, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debate_tweets = pd.read_csv('gop_debate_tweets.csv')\n",
    "debate_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tweet_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @NancyLeeGrahn: How did everyone feel about...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @ScottWalker: Didn't catch the full #GOPdeb...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @TJMShow: No mention of Tamir Rice and the ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @RobGeorge: That Carly Fiorina is trending ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @DanScavino: #GOPDebate w/ @realDonaldTrump...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text tweet_sentiment\n",
       "0  RT @NancyLeeGrahn: How did everyone feel about...         neutral\n",
       "1  RT @ScottWalker: Didn't catch the full #GOPdeb...        positive\n",
       "2  RT @TJMShow: No mention of Tamir Rice and the ...         neutral\n",
       "3  RT @RobGeorge: That Carly Fiorina is trending ...        positive\n",
       "4  RT @DanScavino: #GOPDebate w/ @realDonaldTrump...        positive"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debate_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = tweets.drop('directed_at', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_tweets = tweets.append(negative_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10155, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_tweets = all_tweets.drop('X', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10155, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_tweets = all_tweets.append(airline_tweets)\n",
    "all_tweets = all_tweets.append(debate_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31984, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ingest(all_tweets):\n",
    "    all_tweets = all_tweets[all_tweets['tweet_text'].isnull() == False]\n",
    "    all_tweets = all_tweets[all_tweets['tweet_sentiment'].isnull() == False]\n",
    "    return all_tweets\n",
    "\n",
    "all_tweets = ingest(all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31983, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15138, 2), (10397, 2), (6448, 2))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets[all_tweets['tweet_sentiment'] == 'negative'].shape, all_tweets[all_tweets['tweet_sentiment'] == 'neutral'].shape, all_tweets[all_tweets['tweet_sentiment'] == 'positive'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ '.@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.'\n",
      " '@swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.'\n",
      " \"@sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp; Matt Mullenweg (Wordpress)\"\n",
      " '@teachntech00 New iPad Apps For #SpeechTherapy And Communication Are Showcased At The #SXSW Conference http://ht.ly/49n4M #iear #edchat #asd'\n",
      " '#SXSW is just starting, #CTIA is around the corner and #googleio is only a hop skip and a jump from there, good time to be an #android fan']\n",
      "\n",
      "['.@wesley83 I 3G iPhone. After 3 hrs tweeting #RISE_Austin, dead! I need upgrade. Plugin stations #SXSW.', '@swonderlin Can wait #iPad 2 also. They sale #SXSW.', \"@sxtxstate great stuff Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp; Matt Mullenweg (Wordpress)\", '@teachntech00 New iPad Apps For #SpeechTherapy And Communication Are Showcased At The #SXSW Conference http://ht.ly/49n4M #iear #edchat #asd', '#SXSW starting, #CTIA around corner #googleio hop skip jump there, good time #android fan']\n"
     ]
    }
   ],
   "source": [
    "def filter_stopwords(tweets):\n",
    "    sw = stopwords.words('english')\n",
    "    processed_tweets = []\n",
    "    for tweet in tweets:\n",
    "        text = ' '.join([word for word in tweet.split() if word not in sw])\n",
    "        processed_tweets.append(text)\n",
    "    \n",
    "    return processed_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_tweets['tweet_text'] = all_tweets['tweet_text'].apply(lambda x: x.lower())\n",
    "#all_tweets['tweet_text'] = all_tweets['tweet_text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','', x)))\n",
    "\n",
    "max_features = 2500\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
    "tweets = filter_stopwords(all_tweets['tweet_text'].values)\n",
    "tokenizer.fit_on_texts(tweets)\n",
    "\n",
    "X = tokenizer.texts_to_sequences(tweets)\n",
    "X = pad_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31983, 50)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    3, 1702,   23,  480,   79,  330,  935,\n",
       "         39, 1268,    3,   41,  628,    1], dtype=int32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = pd.get_dummies(all_tweets['tweet_sentiment']).values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28784, 50), (28784, 3), (3199, 50), (3199, 3))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0, 1521, 2289, 1549, 1003,   14,   19,    8,\n",
       "         57,    4,  770,  124,    1,   10], dtype=int32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1], dtype=uint8)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three models: Simple Neural Network, CNN, and LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single hidden layer NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_dim = 32\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_dim, input_length=X_train.shape[1]))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 50, 32)            80000     \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 50, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 100)               160100    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 240,803\n",
      "Trainable params: 240,603\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25905 samples, validate on 2879 samples\n",
      "Epoch 1/30\n",
      "Epoch 00000: val_loss improved from inf to 0.88193, saving model to tweet_sentiment_simple\n",
      "3s - loss: 1.0199 - acc: 0.5345 - val_loss: 0.8819 - val_acc: 0.6342\n",
      "Epoch 2/30\n",
      "Epoch 00001: val_loss improved from 0.88193 to 0.76490, saving model to tweet_sentiment_simple\n",
      "1s - loss: 0.8221 - acc: 0.6332 - val_loss: 0.7649 - val_acc: 0.6447\n",
      "Epoch 3/30\n",
      "Epoch 00002: val_loss improved from 0.76490 to 0.74209, saving model to tweet_sentiment_simple\n",
      "1s - loss: 0.7748 - acc: 0.6456 - val_loss: 0.7421 - val_acc: 0.6468\n",
      "Epoch 4/30\n",
      "Epoch 00003: val_loss improved from 0.74209 to 0.72918, saving model to tweet_sentiment_simple\n",
      "1s - loss: 0.7502 - acc: 0.6597 - val_loss: 0.7292 - val_acc: 0.6745\n",
      "Epoch 5/30\n",
      "Epoch 00004: val_loss improved from 0.72918 to 0.70953, saving model to tweet_sentiment_simple\n",
      "1s - loss: 0.7220 - acc: 0.6816 - val_loss: 0.7095 - val_acc: 0.6891\n",
      "Epoch 6/30\n",
      "Epoch 00005: val_loss improved from 0.70953 to 0.70093, saving model to tweet_sentiment_simple\n",
      "1s - loss: 0.6936 - acc: 0.6984 - val_loss: 0.7009 - val_acc: 0.6982\n",
      "Epoch 7/30\n",
      "Epoch 00006: val_loss improved from 0.70093 to 0.69407, saving model to tweet_sentiment_simple\n",
      "1s - loss: 0.6768 - acc: 0.7102 - val_loss: 0.6941 - val_acc: 0.7016\n",
      "Epoch 8/30\n",
      "Epoch 00007: val_loss improved from 0.69407 to 0.68683, saving model to tweet_sentiment_simple\n",
      "1s - loss: 0.6612 - acc: 0.7192 - val_loss: 0.6868 - val_acc: 0.7110\n",
      "Epoch 9/30\n",
      "Epoch 00008: val_loss did not improve\n",
      "1s - loss: 0.6387 - acc: 0.7299 - val_loss: 0.6924 - val_acc: 0.7023\n",
      "Epoch 10/30\n",
      "Epoch 00009: val_loss did not improve\n",
      "1s - loss: 0.6308 - acc: 0.7358 - val_loss: 0.7118 - val_acc: 0.6975\n",
      "Epoch 11/30\n",
      "Epoch 00010: val_loss improved from 0.68683 to 0.68486, saving model to tweet_sentiment_simple\n",
      "1s - loss: 0.6260 - acc: 0.7341 - val_loss: 0.6849 - val_acc: 0.7121\n",
      "Epoch 12/30\n",
      "Epoch 00011: val_loss did not improve\n",
      "1s - loss: 0.6142 - acc: 0.7452 - val_loss: 0.7010 - val_acc: 0.6971\n",
      "Epoch 13/30\n",
      "Epoch 00012: val_loss did not improve\n",
      "1s - loss: 0.6109 - acc: 0.7452 - val_loss: 0.6930 - val_acc: 0.7065\n",
      "Epoch 14/30\n",
      "Epoch 00013: val_loss did not improve\n",
      "1s - loss: 0.5983 - acc: 0.7514 - val_loss: 0.7034 - val_acc: 0.6989\n",
      "Model took 29.07 seconds to train\n",
      "2336/3199 [====================>.........] - ETA: 0s[0.66700500367245996, 0.71397311641261441]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "model_checkpoint = ModelCheckpoint(filepath='tweet_sentiment_simple', verbose=1, save_best_only=True)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, validation_split=0.1, epochs=30, verbose=2, \n",
    "          callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "end = time.time()\n",
    "print(\"Model took %0.2f seconds to train\"%(end - start))\n",
    "\n",
    "# Re-instantiate model to the best model saved\n",
    "model = load_model('tweet_sentiment_simple')\n",
    "\n",
    "y_pred = model.predict(X_test, batch_size=64)\n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single conv layer with max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(max_features, embed_dim, input_length=X_train.shape[1]))\n",
    "model2.add(SpatialDropout1D(0.3))\n",
    "model2.add(Dropout(0.6))\n",
    "model2.add(Conv1D(32, 5, padding='same', activation='relu'))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(MaxPool1D(5))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(100, activation='relu'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Dropout(0.7))\n",
    "model2.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 50, 128)           320000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_6 (Spatial (None, 50, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 50, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 50, 32)            20512     \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 50, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 10, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 320)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 100)               32100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 373,315\n",
      "Trainable params: 373,115\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25905 samples, validate on 2879 samples\n",
      "Epoch 1/30\n",
      "Epoch 00000: val_loss improved from inf to 0.87728, saving model to tweet_sentiment_cnn\n",
      "9s - loss: 1.1141 - acc: 0.5120 - val_loss: 0.8773 - val_acc: 0.6266\n",
      "Epoch 2/30\n",
      "Epoch 00001: val_loss improved from 0.87728 to 0.79618, saving model to tweet_sentiment_cnn\n",
      "8s - loss: 0.8079 - acc: 0.6344 - val_loss: 0.7962 - val_acc: 0.6499\n",
      "Epoch 3/30\n",
      "Epoch 00002: val_loss improved from 0.79618 to 0.78655, saving model to tweet_sentiment_cnn\n",
      "10s - loss: 0.7644 - acc: 0.6486 - val_loss: 0.7865 - val_acc: 0.6450\n",
      "Epoch 4/30\n",
      "Epoch 00003: val_loss improved from 0.78655 to 0.77921, saving model to tweet_sentiment_cnn\n",
      "8s - loss: 0.7441 - acc: 0.6540 - val_loss: 0.7792 - val_acc: 0.6464\n",
      "Epoch 5/30\n",
      "Epoch 00004: val_loss improved from 0.77921 to 0.75011, saving model to tweet_sentiment_cnn\n",
      "8s - loss: 0.7207 - acc: 0.6710 - val_loss: 0.7501 - val_acc: 0.6714\n",
      "Epoch 6/30\n",
      "Epoch 00005: val_loss improved from 0.75011 to 0.73283, saving model to tweet_sentiment_cnn\n",
      "8s - loss: 0.6938 - acc: 0.6985 - val_loss: 0.7328 - val_acc: 0.6884\n",
      "Epoch 7/30\n",
      "Epoch 00006: val_loss improved from 0.73283 to 0.71645, saving model to tweet_sentiment_cnn\n",
      "9s - loss: 0.6681 - acc: 0.7168 - val_loss: 0.7164 - val_acc: 0.6989\n",
      "Epoch 8/30\n",
      "Epoch 00007: val_loss improved from 0.71645 to 0.71487, saving model to tweet_sentiment_cnn\n",
      "9s - loss: 0.6514 - acc: 0.7243 - val_loss: 0.7149 - val_acc: 0.6985\n",
      "Epoch 9/30\n",
      "Epoch 00008: val_loss improved from 0.71487 to 0.70040, saving model to tweet_sentiment_cnn\n",
      "9s - loss: 0.6386 - acc: 0.7319 - val_loss: 0.7004 - val_acc: 0.7061\n",
      "Epoch 10/30\n",
      "Epoch 00009: val_loss did not improve\n",
      "9s - loss: 0.6304 - acc: 0.7373 - val_loss: 0.7095 - val_acc: 0.7058\n",
      "Epoch 11/30\n",
      "Epoch 00010: val_loss improved from 0.70040 to 0.69945, saving model to tweet_sentiment_cnn\n",
      "8s - loss: 0.6185 - acc: 0.7428 - val_loss: 0.6994 - val_acc: 0.7037\n",
      "Epoch 12/30\n",
      "Epoch 00011: val_loss did not improve\n",
      "8s - loss: 0.6122 - acc: 0.7467 - val_loss: 0.7034 - val_acc: 0.7002\n",
      "Epoch 13/30\n",
      "Epoch 00012: val_loss did not improve\n",
      "8s - loss: 0.6055 - acc: 0.7482 - val_loss: 0.7005 - val_acc: 0.7072\n",
      "Epoch 14/30\n",
      "Epoch 00013: val_loss improved from 0.69945 to 0.69241, saving model to tweet_sentiment_cnn\n",
      "8s - loss: 0.5925 - acc: 0.7548 - val_loss: 0.6924 - val_acc: 0.6989\n",
      "Epoch 15/30\n",
      "Epoch 00014: val_loss did not improve\n",
      "8s - loss: 0.5959 - acc: 0.7531 - val_loss: 0.6971 - val_acc: 0.7009\n",
      "Epoch 16/30\n",
      "Epoch 00015: val_loss did not improve\n",
      "8s - loss: 0.5826 - acc: 0.7607 - val_loss: 0.6977 - val_acc: 0.7002\n",
      "Epoch 17/30\n",
      "Epoch 00016: val_loss did not improve\n",
      "8s - loss: 0.5793 - acc: 0.7622 - val_loss: 0.6959 - val_acc: 0.6995\n",
      "Model took 154.13 seconds to train\n",
      "2976/3199 [==========================>...] - ETA: 0s[0.68082295806753002, 0.70303219737541534]\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "model_checkpoint = ModelCheckpoint(filepath='tweet_sentiment_cnn', verbose=1, save_best_only=True)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "model2.fit(X_train, y_train, batch_size=64, validation_split=0.1, epochs=30, verbose=2, \n",
    "          callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "end = time.time()\n",
    "print(\"Model took %0.2f seconds to train\"%(end - start))\n",
    "\n",
    "# Re-instantiate model to the best model saved\n",
    "model2 = load_model('tweet_sentiment_cnn')\n",
    "\n",
    "y_pred = model2.predict(X_test, batch_size=64)\n",
    "score = model2.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_dim = 32\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Embedding(max_features, embed_dim, input_length=X_train.shape[1], embeddings_regularizer=l2(0.0001)))\n",
    "model3.add(SpatialDropout1D(0.2))\n",
    "model3.add(Dropout(0.6))\n",
    "\n",
    "model3.add(Conv1D(128, 5, activation='relu'))\n",
    "model3.add(Dropout(0.6))\n",
    "model3.add(MaxPool1D())\n",
    "\n",
    "model3.add(Flatten())\n",
    "\n",
    "model3.add(Dense(256, activation='relu', kernel_regularizer=l2(0.0001)))\n",
    "model3.add(Dropout(0.7))\n",
    "model3.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 50, 32)            80000     \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_7 (Spatial (None, 50, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 50, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 46, 128)           20608     \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 46, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 23, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 2944)              0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 256)               753920    \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 855,299\n",
      "Trainable params: 855,299\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model3.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001, decay=1e-7), metrics=['accuracy'])\n",
    "model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25905 samples, validate on 2879 samples\n",
      "Epoch 1/30\n",
      "Epoch 00000: val_loss improved from inf to 0.81498, saving model to tweet_sentiment_multicnn\n",
      "11s - loss: 0.9122 - acc: 0.5905 - val_loss: 0.8150 - val_acc: 0.6495\n",
      "Epoch 2/30\n",
      "Epoch 00001: val_loss improved from 0.81498 to 0.78907, saving model to tweet_sentiment_multicnn\n",
      "10s - loss: 0.7825 - acc: 0.6514 - val_loss: 0.7891 - val_acc: 0.6464\n",
      "Epoch 3/30\n",
      "Epoch 00002: val_loss improved from 0.78907 to 0.77503, saving model to tweet_sentiment_multicnn\n",
      "10s - loss: 0.7587 - acc: 0.6615 - val_loss: 0.7750 - val_acc: 0.6471\n",
      "Epoch 4/30\n",
      "Epoch 00003: val_loss improved from 0.77503 to 0.74506, saving model to tweet_sentiment_multicnn\n",
      "10s - loss: 0.7345 - acc: 0.6883 - val_loss: 0.7451 - val_acc: 0.6888\n",
      "Epoch 5/30\n",
      "Epoch 00004: val_loss improved from 0.74506 to 0.73621, saving model to tweet_sentiment_multicnn\n",
      "10s - loss: 0.7121 - acc: 0.7102 - val_loss: 0.7362 - val_acc: 0.6950\n",
      "Epoch 6/30\n",
      "Epoch 00005: val_loss improved from 0.73621 to 0.72693, saving model to tweet_sentiment_multicnn\n",
      "10s - loss: 0.6945 - acc: 0.7232 - val_loss: 0.7269 - val_acc: 0.6978\n",
      "Epoch 7/30\n",
      "Epoch 00006: val_loss did not improve\n",
      "10s - loss: 0.6862 - acc: 0.7284 - val_loss: 0.7356 - val_acc: 0.6933\n",
      "Epoch 8/30\n",
      "Epoch 00007: val_loss did not improve\n",
      "10s - loss: 0.6775 - acc: 0.7351 - val_loss: 0.7320 - val_acc: 0.6971\n",
      "Epoch 9/30\n",
      "Epoch 00008: val_loss did not improve\n",
      "11s - loss: 0.6745 - acc: 0.7351 - val_loss: 0.7313 - val_acc: 0.6978\n",
      "Model took 96.45 seconds to train\n",
      "2912/3199 [==========================>...] - ETA: 0s[0.71138857388429322, 0.70240700229997743]\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "model_checkpoint = ModelCheckpoint(filepath='tweet_sentiment_multicnn', verbose=1, save_best_only=True)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "model3.fit(X_train, y_train, batch_size=64, validation_split=0.1, epochs=30, verbose=2, \n",
    "          callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "end = time.time()\n",
    "print(\"Model took %0.2f seconds to train\"%(end - start))\n",
    "\n",
    "# Re-instantiate model to the best model saved\n",
    "model3 = load_model('tweet_sentiment_multicnn')\n",
    "\n",
    "y_pred = model3.predict(X_test, batch_size=64)\n",
    "score = model3.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 50, 96)            240000    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 50, 96)            384       \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 50, 96)            0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 96)                74112     \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 125)               12125     \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 125)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 3)                 378       \n",
      "=================================================================\n",
      "Total params: 326,999\n",
      "Trainable params: 326,807\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 96\n",
    "lstm_out = 96\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Embedding(max_features, embed_dim, input_length = X.shape[1]))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(LSTM(lstm_out, dropout=0.4, recurrent_dropout=0.3, activation='relu'))\n",
    "model3.add(Dense(125, activation='relu'))\n",
    "model3.add(Dropout(0.7))\n",
    "model3.add(Dense(3, activation='softmax'))\n",
    "model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25905 samples, validate on 2879 samples\n",
      "Epoch 1/30\n",
      "Epoch 00000: val_loss improved from inf to 1.01078, saving model to tweet_sentiment_lstm\n",
      "49s - loss: 0.9024 - acc: 0.5924 - val_loss: 1.0108 - val_acc: 0.6096\n",
      "Epoch 2/30\n",
      "Epoch 00001: val_loss improved from 1.01078 to 0.75497, saving model to tweet_sentiment_lstm\n",
      "47s - loss: 0.7893 - acc: 0.6416 - val_loss: 0.7550 - val_acc: 0.6659\n",
      "Epoch 3/30\n",
      "Epoch 00002: val_loss improved from 0.75497 to 0.72229, saving model to tweet_sentiment_lstm\n",
      "50s - loss: 0.7454 - acc: 0.6753 - val_loss: 0.7223 - val_acc: 0.6732\n",
      "Epoch 4/30\n",
      "Epoch 00003: val_loss improved from 0.72229 to 0.70923, saving model to tweet_sentiment_lstm\n",
      "49s - loss: 0.7267 - acc: 0.6884 - val_loss: 0.7092 - val_acc: 0.6787\n",
      "Epoch 5/30\n",
      "Epoch 00004: val_loss improved from 0.70923 to 0.69887, saving model to tweet_sentiment_lstm\n",
      "47s - loss: 0.7094 - acc: 0.6970 - val_loss: 0.6989 - val_acc: 0.6801\n",
      "Epoch 6/30\n",
      "Epoch 00005: val_loss improved from 0.69887 to 0.69239, saving model to tweet_sentiment_lstm\n",
      "49s - loss: 0.6920 - acc: 0.7048 - val_loss: 0.6924 - val_acc: 0.6843\n",
      "Epoch 7/30\n",
      "Epoch 00006: val_loss improved from 0.69239 to 0.68856, saving model to tweet_sentiment_lstm\n",
      "50s - loss: 0.6815 - acc: 0.7067 - val_loss: 0.6886 - val_acc: 0.6829\n",
      "Epoch 8/30\n",
      "Epoch 00007: val_loss did not improve\n",
      "51s - loss: 0.6740 - acc: 0.7084 - val_loss: 0.6915 - val_acc: 0.6891\n",
      "Epoch 9/30\n",
      "Epoch 00008: val_loss did not improve\n",
      "48s - loss: 0.6669 - acc: 0.7101 - val_loss: 0.6948 - val_acc: 0.6811\n",
      "Epoch 10/30\n",
      "Epoch 00009: val_loss did not improve\n",
      "47s - loss: 0.6631 - acc: 0.7138 - val_loss: 0.6907 - val_acc: 0.6870\n",
      "Model took 493.73 seconds to train\n",
      "3168/3199 [============================>.] - ETA: 0s[0.66116661912167429, 0.70553297886963229]\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "model_checkpoint = ModelCheckpoint(filepath='tweet_sentiment_lstm', verbose=1, save_best_only=True)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "model3.fit(X_train, y_train, batch_size=64, validation_split=0.1, epochs=30, verbose=2, \n",
    "          callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "end = time.time()\n",
    "print(\"Model took %0.2f seconds to train\"%(end - start))\n",
    "\n",
    "# Re-instantiate model to the best model saved\n",
    "model3 = load_model('tweet_sentiment_lstm')\n",
    "\n",
    "y_pred = model3.predict(X_test, batch_size=64)\n",
    "score = model3.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Pre-trained Model (GloVe-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join('glove.twitter.27B', 'glove.twitter.27B.25d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34043 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_dim = 32\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embed_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
